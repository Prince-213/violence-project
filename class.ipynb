{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1: Normal (0.00)\n",
      "Sequence 2: Normal (0.00)\n",
      "Sequence 3: Normal (0.00)\n",
      "Sequence 4: Normal (0.00)\n",
      "Sequence 5: Normal (0.00)\n",
      "Sequence 6: Normal (0.00)\n",
      "Sequence 7: Normal (0.00)\n",
      "Sequence 8: Normal (0.00)\n",
      "Sequence 9: Normal (0.00)\n",
      "Sequence 10: Normal (0.00)\n",
      "Sequence 11: Normal (0.00)\n",
      "Sequence 12: Normal (0.00)\n",
      "Sequence 13: Normal (0.00)\n",
      "Sequence 14: Normal (0.00)\n",
      "Sequence 15: Normal (0.00)\n",
      "Sequence 16: Normal (0.00)\n",
      "Sequence 17: Normal (0.00)\n",
      "Sequence 18: Normal (0.00)\n",
      "Sequence 19: Normal (0.00)\n",
      "Sequence 20: Normal (0.00)\n",
      "Sequence 21: Normal (0.00)\n",
      "Sequence 22: Normal (0.00)\n",
      "Sequence 23: Normal (0.00)\n",
      "Sequence 24: Normal (0.00)\n",
      "Sequence 25: Normal (0.00)\n",
      "Sequence 26: Normal (0.00)\n",
      "Sequence 27: Normal (0.00)\n",
      "Sequence 28: Normal (0.00)\n",
      "Sequence 29: Normal (0.00)\n",
      "Sequence 30: Normal (0.00)\n",
      "Sequence 31: Normal (0.00)\n",
      "Sequence 32: Normal (0.00)\n",
      "Sequence 33: Normal (0.00)\n",
      "Sequence 34: Normal (0.00)\n",
      "Sequence 35: Normal (0.00)\n",
      "Sequence 36: Normal (0.00)\n",
      "Sequence 37: Normal (0.00)\n",
      "Sequence 38: Normal (0.00)\n",
      "Sequence 39: Normal (0.00)\n",
      "Sequence 40: Normal (0.00)\n",
      "Sequence 41: Normal (0.00)\n",
      "Sequence 42: Normal (0.00)\n",
      "Sequence 43: Normal (0.00)\n",
      "Sequence 44: Normal (0.00)\n",
      "Sequence 45: Normal (0.00)\n",
      "Sequence 46: Normal (0.00)\n",
      "Sequence 47: Normal (0.00)\n",
      "Sequence 48: Normal (0.00)\n",
      "Sequence 49: Normal (0.00)\n",
      "Sequence 50: Normal (0.00)\n",
      "Sequence 51: Normal (0.00)\n",
      "Sequence 52: Normal (0.00)\n",
      "Sequence 53: Normal (0.00)\n",
      "Sequence 54: Normal (0.00)\n",
      "Sequence 55: Normal (0.00)\n",
      "Sequence 56: Normal (0.00)\n",
      "Sequence 57: Normal (0.00)\n",
      "Sequence 58: Normal (0.00)\n",
      "Sequence 59: Normal (0.00)\n",
      "Sequence 60: Normal (0.00)\n",
      "Sequence 61: Normal (0.00)\n",
      "Sequence 62: Normal (0.00)\n",
      "Sequence 63: Normal (0.00)\n",
      "Sequence 64: Normal (0.00)\n",
      "Sequence 65: Normal (0.00)\n",
      "Sequence 66: Normal (0.00)\n",
      "Sequence 67: Normal (0.00)\n",
      "Sequence 68: Normal (0.00)\n",
      "Sequence 69: Normal (0.00)\n",
      "Sequence 70: Normal (0.00)\n",
      "Sequence 71: Normal (0.00)\n",
      "Sequence 72: Normal (0.00)\n",
      "Sequence 73: Normal (0.00)\n",
      "Sequence 74: Normal (0.00)\n",
      "Sequence 75: Normal (0.00)\n",
      "Sequence 76: Normal (0.00)\n",
      "Sequence 77: Normal (0.00)\n",
      "Sequence 78: Normal (0.00)\n",
      "Sequence 79: Normal (0.00)\n",
      "Sequence 80: Normal (0.00)\n",
      "Sequence 81: Normal (0.00)\n",
      "Sequence 82: Normal (0.00)\n",
      "Sequence 83: Normal (0.00)\n",
      "Sequence 84: Normal (0.00)\n",
      "Sequence 85: Normal (0.00)\n",
      "Sequence 86: Normal (0.00)\n",
      "Sequence 87: Normal (0.00)\n",
      "Sequence 88: Normal (0.00)\n",
      "Sequence 89: Normal (0.00)\n",
      "Sequence 90: Normal (0.00)\n",
      "Sequence 91: Normal (0.00)\n",
      "Sequence 92: Normal (0.00)\n",
      "Sequence 93: Normal (0.00)\n",
      "Sequence 94: Normal (0.00)\n",
      "Sequence 95: Normal (0.00)\n",
      "Sequence 96: Normal (0.00)\n",
      "Sequence 97: Normal (0.00)\n",
      "Sequence 98: Normal (0.00)\n",
      "Sequence 99: Normal (0.00)\n",
      "Sequence 100: Normal (0.00)\n",
      "Sequence 101: Normal (0.00)\n",
      "Sequence 102: Normal (0.00)\n",
      "Sequence 103: Normal (0.00)\n",
      "Sequence 104: Normal (0.00)\n",
      "Sequence 105: Normal (0.00)\n",
      "Sequence 106: Normal (0.00)\n",
      "Sequence 107: Normal (0.00)\n",
      "Sequence 108: Normal (0.00)\n",
      "Sequence 109: Normal (0.00)\n",
      "Sequence 110: Normal (0.00)\n",
      "Sequence 111: Normal (0.00)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"./violence_detection_model.h5\")\n",
    "class_names = ['Normal', 'Violence', 'Weapon']\n",
    "input_shape = model.input_shape[1:]  # Should be (15, 128, 128, 3)\n",
    "\n",
    "def create_frame_sequence(video_path, seq_length=15):\n",
    "    \"\"\"Extracts sequences of frames from video\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    sequences = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess each frame\n",
    "        frame = cv2.resize(frame, (input_shape[2], input_shape[1]))  # (128, 128)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = frame / 255.0  # Normalize\n",
    "        \n",
    "        frames.append(frame)\n",
    "        \n",
    "        # When we have enough frames for a sequence\n",
    "        if len(frames) == seq_length:\n",
    "            sequences.append(np.array(frames))\n",
    "            frames = frames[1:]  # Sliding window (optional)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(sequences)\n",
    "\n",
    "def classify_video(video_path):\n",
    "    \"\"\"Classifies video using 3D model\"\"\"\n",
    "    # Create sequences of 15 frames each\n",
    "    sequences = create_frame_sequence(video_path)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        print(\"Not enough frames in video\")\n",
    "        return\n",
    "    \n",
    "    # Process each sequence\n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Add batch dimension (1, 15, 128, 128, 3)\n",
    "        seq = np.expand_dims(seq, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(seq, verbose=0)\n",
    "        pred_class = class_names[np.argmax(predictions[0])]\n",
    "        confidence = np.max(predictions[0])\n",
    "        \n",
    "        print(f\"Sequence {i+1}: {pred_class} ({confidence:.2f})\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"./t_n001_converted.avi\"\n",
    "classify_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input shape: (None, 15, 128, 128, 3)\n",
      "Model output shape: (None, 1)\n",
      "Full prediction array: [[0.97073334]]\n",
      "Warning: Expected 3 outputs, got 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m    115\u001b[39m video_path = \u001b[33m\"\u001b[39m\u001b[33m./t_v011_converted.avi\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[43mclassify_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mclassify_video\u001b[39m\u001b[34m(video_path, sequence_length)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (class_name, prob) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(class_names, current_predictions)):\n\u001b[32m     76\u001b[39m     color = (\u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m ((\u001b[32m0\u001b[39m, \u001b[32m165\u001b[39m, \u001b[32m255\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     cv2.putText(display_frame, text, (\u001b[32m20\u001b[39m, y_offset), cv2.FONT_HERSHEY_SIMPLEX,\n\u001b[32m     79\u001b[39m                \u001b[32m0.7\u001b[39m, color, \u001b[32m2\u001b[39m, cv2.LINE_AA)\n\u001b[32m     80\u001b[39m     y_offset += \u001b[32m30\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# Load model and verify output shape\n",
    "model = load_model(\"./violence_detection_model.h5\")\n",
    "print(\"Model input shape:\", model.input_shape)\n",
    "print(\"Model output shape:\", model.output_shape)  # Should be (None, 3) for 3 classes\n",
    "\n",
    "class_names = ['Normal', 'Violence', 'Weaponized']\n",
    "input_shape = model.input_shape[1:]  # Should be (15, 128, 128, 3)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"classified_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Process individual frame to model input specs\"\"\"\n",
    "    frame = cv2.resize(frame, (input_shape[2], input_shape[1]))  # Resize to 128x128\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    frame = frame / 255.0  # Normalize to [0,1]\n",
    "    return frame\n",
    "\n",
    "def classify_video(video_path, sequence_length=15):\n",
    "    \"\"\"Main processing function with complete prediction handling\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    frame_buffer = deque(maxlen=sequence_length)\n",
    "    frame_count = 0\n",
    "    current_predictions = [0, 0, 0]  # Initialize all class probabilities\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(os.path.join(output_dir, 'output_video.mp4'), fourcc, fps, (width, height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process frame and add to buffer\n",
    "        processed_frame = process_frame(frame)\n",
    "        frame_buffer.append(processed_frame)\n",
    "        \n",
    "        # When we have enough frames for a sequence\n",
    "        if len(frame_buffer) == sequence_length:\n",
    "            sequence = np.expand_dims(np.array(frame_buffer), axis=0)\n",
    "            \n",
    "            # Get full prediction array\n",
    "            predictions = model.predict(sequence, verbose=0)  # Get first (and only) batch item\n",
    "            print(\"Full prediction array:\", predictions)  # Debug output\n",
    "            \n",
    "            # Verify prediction array length matches classes\n",
    "            if len(predictions) != len(class_names):\n",
    "                print(f\"Warning: Expected {len(class_names)} outputs, got {len(predictions)}\")\n",
    "                current_predictions = predictions  # Store whatever we got\n",
    "            else:\n",
    "                current_predictions = predictions\n",
    "        \n",
    "        # Prepare display frame with all prediction info\n",
    "        display_frame = frame.copy()\n",
    "        \n",
    "        # Display all class probabilities\n",
    "        y_offset = 40\n",
    "        for i, (class_name, prob) in enumerate(zip(class_names, current_predictions)):\n",
    "            color = (0, 255, 0) if i == 0 else ((0, 165, 255) if i == 1 else (0, 0, 255))\n",
    "            text = f\"{class_name}: {prob:.4f}\"\n",
    "            cv2.putText(display_frame, text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                       0.7, color, 2, cv2.LINE_AA)\n",
    "            y_offset += 30\n",
    "        \n",
    "        # Highlight the highest probability class\n",
    "        dominant_class = np.argmax(current_predictions)\n",
    "        cv2.rectangle(display_frame, (10, 10), (50, 50), \n",
    "                     (0, 255, 0) if dominant_class == 0 else \n",
    "                     (0, 165, 255) if dominant_class == 1 else \n",
    "                     (0, 0, 255), -1)\n",
    "        \n",
    "        # Add frame counter\n",
    "        cv2.putText(display_frame, f\"Frame: {frame_count}\", \n",
    "                   (20, y_offset + 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   0.7, (200, 200, 0), 2)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Violence Detection', display_frame)\n",
    "        \n",
    "        # Save frame\n",
    "        output_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(output_path, display_frame)\n",
    "        \n",
    "        # Write to output video\n",
    "        out.write(display_frame)\n",
    "        \n",
    "        # Control playback\n",
    "        frame_count += 1\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Processing complete. Saved {frame_count} frames to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"./t_v011_converted.avi\"\n",
    "classify_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')  # Before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "def print_results(video, limit=None):\n",
    "        fig=plt.figure(figsize=(16, 30))\n",
    "        if not os.path.exists('output'):\n",
    "            os.mkdir('output')\n",
    "\n",
    "        print(\"Loading model ...\")\n",
    "        model = load_model('./violence_detection_model.h5')\n",
    "        Q = deque(maxlen=128)\n",
    "\n",
    "        vs = cv2.VideoCapture(video)\n",
    "        writer = None\n",
    "        (W, H) = (None, None)\n",
    "        count = 0     \n",
    "        while True:\n",
    "                (grabbed, frame) = vs.read()\n",
    "                ID = vs.get(1)\n",
    "                if not grabbed:\n",
    "                    break\n",
    "                try:\n",
    "                    if (ID % 7 == 0):\n",
    "                        count = count + 1\n",
    "                        n_frames = len(frame)\n",
    "                        \n",
    "                        if W is None or H is None:\n",
    "                            (H, W) = frame.shape[:2]\n",
    "\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        output = cv2.resize(frame, (512, 360)).copy()\n",
    "                        frame = cv2.resize(frame, (128, 128)).astype(\"float32\")\n",
    "                        frame = frame.reshape(IMG_SIZE, IMG_SIZE, 3) / 255\n",
    "                        preds = model.predict(np.expand_dims(frame, axis=0))[0]\n",
    "                        Q.append(preds)\n",
    "\n",
    "                        results = np.array(Q).mean(axis=0)\n",
    "                        i = (preds > 0.56)[0] #np.argmax(results)\n",
    "\n",
    "                        label = i\n",
    "\n",
    "                        text = \"Violence: {}\".format(label)\n",
    "                        #print('prediction:', text)\n",
    "                        file = open(\"output.txt\",'w')\n",
    "                        file.write(text)\n",
    "                        file.close()\n",
    "\n",
    "                        color = (0, 255, 0)\n",
    "\n",
    "                        if label:\n",
    "                            color = (255, 0, 0) \n",
    "                        else:\n",
    "                            color = (0, 255, 0)\n",
    "\n",
    "                        cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                1, color, 3)\n",
    "\n",
    "\n",
    "                        # saving mp4 with labels but cv2.imshow is not working with this notebook\n",
    "                        if writer is None:\n",
    "                                fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "                                writer = cv2.VideoWriter(\"output.mp4\", fourcc, 60,\n",
    "                                        (W, H), True)\n",
    "\n",
    "                        writer.write(output)\n",
    "                        #cv2.imshow(\"Output\", output)\n",
    "\n",
    "                        fig.add_subplot(8, 3, count)\n",
    "                        plt.imshow(output)\n",
    "\n",
    "                    if limit and count > limit:\n",
    "                        break\n",
    "\n",
    "                except:\n",
    "                    break \n",
    "        \n",
    "        plt.show()\n",
    "        print(\"Cleaning up...\")\n",
    "        if writer is not None:\n",
    "            writer.release()\n",
    "        vs.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 171 frames\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "\n",
    "# Constants\n",
    "SEQ_LENGTH = 15  # Number of frames in each sequence\n",
    "IMG_SIZE = 128\n",
    "DISPLAY_SIZE = (512, 360)\n",
    "VIOLENCE_THRESHOLD = 0.56  # Confidence threshold for violence detection\n",
    "\n",
    "def process_full_video(video_path):\n",
    "    # Create output directory\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model('./violence_detection_model.h5')\n",
    "    \n",
    "    # Initialize frame buffer\n",
    "    frame_buffer = deque(maxlen=SEQ_LENGTH)\n",
    "    predictions = deque(maxlen=SEQ_LENGTH)  # Store recent predictions\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties for writer\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter('output.mp4', fourcc, fps, DISPLAY_SIZE)\n",
    "    \n",
    "    # Create display window\n",
    "    cv2.namedWindow(\"Violence Detection\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Violence Detection\", *DISPLAY_SIZE)\n",
    "    \n",
    "    frame_count = 0\n",
    "    current_status = \"Initializing...\"\n",
    "    current_confidence = 0.0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Process frame\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            display_frame = cv2.resize(frame_rgb, DISPLAY_SIZE)\n",
    "            \n",
    "            # Prepare frame for model\n",
    "            model_frame = cv2.resize(frame_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "            model_frame = model_frame.astype(\"float32\") / 255.0\n",
    "            frame_buffer.append(model_frame)\n",
    "            \n",
    "            # When we have enough frames for a sequence\n",
    "            if len(frame_buffer) == SEQ_LENGTH:\n",
    "                # Create sequence tensor (1, 15, 128, 128, 3)\n",
    "                sequence = np.expand_dims(np.array(frame_buffer), axis=0)\n",
    "                \n",
    "                # Make prediction\n",
    "                pred = model.predict(sequence, verbose=0)[0]\n",
    "                predictions.append(pred)\n",
    "                \n",
    "                # Apply temporal smoothing (average of last SEQ_LENGTH predictions)\n",
    "                avg_prediction = np.mean(predictions) if predictions else 0.0\n",
    "                is_violence = avg_prediction > VIOLENCE_THRESHOLD\n",
    "                current_status = \"Violence\" if is_violence else \"Normal\"\n",
    "                current_confidence = avg_prediction\n",
    "            \n",
    "            # Add annotation to display frame (using current status)\n",
    "            color = (0, 0, 255) if current_status == \"Violence\" else (0, 255, 0)\n",
    "            cv2.putText(display_frame, \n",
    "                       f\"{current_status} ({current_confidence:.2f})\", \n",
    "                       (35, 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, color, 2)\n",
    "            \n",
    "            # Save text output (continuously updating)\n",
    "            with open(\"output.txt\", 'w') as f:\n",
    "                f.write(f\"{current_status} ({current_confidence:.2f})\")\n",
    "            \n",
    "            # Write and display frame\n",
    "            writer.write(cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imshow(\"Violence Detection\", cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            frame_count += 1\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Finished processing {frame_count} frames\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"./video/w006_converted.avi\"\n",
    "process_full_video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TKINTER W 2 CLASS NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "import threading\n",
    "\n",
    "# Constants\n",
    "SEQ_LENGTH = 15\n",
    "IMG_SIZE = 128\n",
    "DISPLAY_SIZE = (640, 360)\n",
    "VIOLENCE_THRESHOLD = 0.56\n",
    "\n",
    "class ViolenceDetectionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Violence Detection System\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        \n",
    "        # Model and video processing variables\n",
    "        self.model = None\n",
    "        self.cap = None\n",
    "        self.running = False\n",
    "        self.frame_buffer = deque(maxlen=SEQ_LENGTH)\n",
    "        self.predictions = deque(maxlen=SEQ_LENGTH)\n",
    "        self.current_status = \"Ready\"\n",
    "        self.current_confidence = 0.0\n",
    "        self.last_prediction = 0.0  # Store the last prediction value\n",
    "        \n",
    "        # Create containers\n",
    "        self.main_frame = tk.Frame(self.root)\n",
    "        self.greeting_frame = self.create_greeting_frame()\n",
    "        self.detection_frame = None\n",
    "        \n",
    "        # Show greeting frame initially\n",
    "        self.show_greeting_frame()\n",
    "        \n",
    "    def create_greeting_frame(self):\n",
    "        frame = tk.Frame(self.main_frame)\n",
    "        \n",
    "        tk.Label(frame, text=\"Welcome to Violence Detection System\", \n",
    "                font=(\"Helvetica\", 18, \"bold\")).pack(pady=20)\n",
    "        \n",
    "        tk.Label(frame, text=\"This application detects violent behavior in video sequences\", \n",
    "                font=(\"Helvetica\", 12)).pack(pady=10)\n",
    "        \n",
    "        tk.Label(frame, text=\"Using deep learning to analyze video frames\", \n",
    "                font=(\"Helvetica\", 12)).pack(pady=10)\n",
    "        \n",
    "        start_button = tk.Button(frame, text=\"Get Started\", command=self.start_detection_flow,\n",
    "                                font=(\"Helvetica\", 14), bg=\"#4CAF50\", fg=\"white\")\n",
    "        start_button.pack(pady=30)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def create_detection_frame(self):\n",
    "        frame = tk.Frame(self.main_frame)\n",
    "        \n",
    "        # Video display\n",
    "        self.video_label = tk.Label(frame)\n",
    "        self.video_label.pack(pady=10)\n",
    "        \n",
    "        # Control panel\n",
    "        control_frame = tk.Frame(frame)\n",
    "        control_frame.pack(pady=10)\n",
    "        \n",
    "        self.select_button = tk.Button(control_frame, text=\"Select Video\", command=self.select_video)\n",
    "        self.select_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.start_button = tk.Button(control_frame, text=\"Start\", command=self.start_detection, \n",
    "                                     state=tk.DISABLED)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.stop_button = tk.Button(control_frame, text=\"Stop\", command=self.stop_detection,\n",
    "                                    state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Status display\n",
    "        self.status_var = tk.StringVar(value=\"Status: Ready\")\n",
    "        tk.Label(frame, textvariable=self.status_var, font=(\"Helvetica\", 12)).pack(pady=5)\n",
    "        \n",
    "        # Confidence display\n",
    "        self.confidence_var = tk.StringVar(value=\"\")\n",
    "        tk.Label(frame, textvariable=self.confidence_var, font=(\"Helvetica\", 12)).pack(pady=5)\n",
    "        \n",
    "        # Back button\n",
    "        tk.Button(frame, text=\"Back to Home\", command=self.show_greeting_frame).pack(pady=10)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def show_greeting_frame(self):\n",
    "        if self.detection_frame:\n",
    "            self.detection_frame.pack_forget()\n",
    "        self.greeting_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "    def start_detection_flow(self):\n",
    "        self.greeting_frame.pack_forget()\n",
    "        if not self.detection_frame:\n",
    "            self.detection_frame = self.create_detection_frame()\n",
    "        self.detection_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Load model in background\n",
    "        threading.Thread(target=self.load_model, daemon=True).start()\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.status_var.set(\"Status: Loading model...\")\n",
    "        try:\n",
    "            self.model = load_model('./violence_detection_model.h5')\n",
    "            self.status_var.set(\"Status: Model loaded. Select a video.\")\n",
    "            self.select_button.config(state=tk.NORMAL)\n",
    "        except Exception as e:\n",
    "            self.status_var.set(f\"Status: Error loading model - {str(e)}\")\n",
    "    \n",
    "    def select_video(self):\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select Video File\",\n",
    "            filetypes=[(\"Video Files\", \"*.mp4 *.avi *.mov\"), (\"All Files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            self.video_path = file_path\n",
    "            self.status_var.set(f\"Status: Selected {os.path.basename(file_path)}\")\n",
    "            self.start_button.config(state=tk.NORMAL)\n",
    "    \n",
    "    def start_detection(self):\n",
    "        if not hasattr(self, 'video_path'):\n",
    "            return\n",
    "            \n",
    "        self.running = True\n",
    "        self.start_button.config(state=tk.DISABLED)\n",
    "        self.stop_button.config(state=tk.NORMAL)\n",
    "        self.select_button.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Reset buffers\n",
    "        self.frame_buffer = deque(maxlen=SEQ_LENGTH)\n",
    "        self.predictions = deque(maxlen=SEQ_LENGTH)\n",
    "        self.current_status = \"Processing...\"\n",
    "        self.current_confidence = 0.0\n",
    "        self.last_prediction = 0.0\n",
    "        \n",
    "        # Start video processing in a separate thread\n",
    "        threading.Thread(target=self.process_video, daemon=True).start()\n",
    "    \n",
    "    def stop_detection(self):\n",
    "        self.running = False\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.select_button.config(state=tk.NORMAL)\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "    \n",
    "    def process_video(self):\n",
    "        self.cap = cv2.VideoCapture(self.video_path)\n",
    "        if not self.cap.isOpened():\n",
    "            self.status_var.set(\"Status: Error opening video file\")\n",
    "            return\n",
    "        \n",
    "        while self.running and self.cap.isOpened():\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Process frame\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                display_frame = cv2.resize(frame_rgb, DISPLAY_SIZE)\n",
    "                \n",
    "                # Prepare frame for model\n",
    "                model_frame = cv2.resize(frame_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "                model_frame = model_frame.astype(\"float32\") / 255.0\n",
    "                self.frame_buffer.append(model_frame)\n",
    "                \n",
    "                # When we have enough frames for a sequence\n",
    "                if len(self.frame_buffer) == SEQ_LENGTH and self.model:\n",
    "                    # Create sequence tensor (1, 15, 128, 128, 3)\n",
    "                    sequence = np.expand_dims(np.array(self.frame_buffer), axis=0)\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    pred = self.model.predict(sequence, verbose=0)[0][0]\n",
    "                    self.predictions.append(pred)\n",
    "                    self.last_prediction = pred  # Store the latest prediction\n",
    "                    \n",
    "                    # Apply temporal smoothing (average of last SEQ_LENGTH predictions)\n",
    "                    if len(self.predictions) > 0:\n",
    "                        avg_prediction = np.mean(self.predictions)\n",
    "                        is_violence = avg_prediction > VIOLENCE_THRESHOLD\n",
    "                        self.current_status = \"Violence\" if is_violence else \"Normal\"\n",
    "                        self.current_confidence = avg_prediction\n",
    "                    else:\n",
    "                        self.current_status = \"Processing...\"\n",
    "                        self.current_confidence = 0.0\n",
    "                else:\n",
    "                    # If we don't have enough frames yet, use the last prediction\n",
    "                    if hasattr(self, 'last_prediction'):\n",
    "                        is_violence = self.last_prediction > VIOLENCE_THRESHOLD\n",
    "                        self.current_status = \"Violence\" if is_violence else \"Normal\"\n",
    "                        self.current_confidence = self.last_prediction\n",
    "                \n",
    "                # Update status display\n",
    "                self.status_var.set(f\"Status: {self.current_status}\")\n",
    "                self.confidence_var.set(f\"Confidence: {self.current_confidence:.2f}\")\n",
    "                \n",
    "                # Add annotation to display frame\n",
    "                color = (255, 0, 0) if self.current_status == \"Violence\" else (0, 255, 0)\n",
    "                cv2.putText(display_frame, \n",
    "                           f\"{self.current_status} ({self.current_confidence:.2f})\", \n",
    "                           (35, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           1, color, 2)\n",
    "                \n",
    "                # Convert to PhotoImage and display\n",
    "                img = Image.fromarray(display_frame)\n",
    "                imgtk = ImageTk.PhotoImage(image=img)\n",
    "                \n",
    "                # Update the display in the main thread\n",
    "                self.root.after(0, self.update_display, imgtk)\n",
    "                \n",
    "                # Control playback speed\n",
    "                cv2.waitKey(25)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.status_var.set(f\"Status: Error processing frame - {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        # Cleanup\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.running = False\n",
    "        self.root.after(0, self.stop_detection)\n",
    "    \n",
    "    def update_display(self, image):\n",
    "        self.video_label.configure(image=image)\n",
    "        self.video_label.image = image  # Keep a reference\n",
    "    \n",
    "    def on_closing(self):\n",
    "        self.stop_detection()\n",
    "        self.root.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ViolenceDetectionApp(root)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class ViolenceDetectionUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Violence Detection System\")\n",
    "        self.root.geometry(\"900x700\")\n",
    "        \n",
    "        # Constants\n",
    "        self.SEQ_LENGTH = 15\n",
    "        self.IMG_SIZE = 64\n",
    "        self.DISPLAY_SIZE = (640, 360)\n",
    "        self.VIOLENCE_THRESHOLD = 0.56\n",
    "        \n",
    "        # Model and video processing variables\n",
    "        self.model = None\n",
    "        self.cap = None\n",
    "        self.running = False\n",
    "        self.frame_buffer = deque(maxlen=self.SEQ_LENGTH)\n",
    "        self.predictions = deque(maxlen=self.SEQ_LENGTH)\n",
    "        self.current_status = \"Ready\"\n",
    "        \n",
    "        # UI Elements\n",
    "        self.create_widgets()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        # Main container\n",
    "        self.main_frame = tk.Frame(self.root, padx=10, pady=10)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Video display\n",
    "        self.video_label = tk.Label(self.main_frame, bg='black')\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Control panel\n",
    "        control_frame = tk.Frame(self.main_frame)\n",
    "        control_frame.pack(fill=tk.X, pady=10)\n",
    "        \n",
    "        # Buttons\n",
    "        self.load_btn = tk.Button(control_frame, text=\"Load Model\", command=self.load_model)\n",
    "        self.load_btn.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.select_btn = tk.Button(control_frame, text=\"Select Video\", command=self.select_video, state=tk.DISABLED)\n",
    "        self.select_btn.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.start_btn = tk.Button(control_frame, text=\"Start\", command=self.start_processing, state=tk.DISABLED)\n",
    "        self.start_btn.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.stop_btn = tk.Button(control_frame, text=\"Stop\", command=self.stop_processing, state=tk.DISABLED)\n",
    "        self.stop_btn.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Status display\n",
    "        status_frame = tk.Frame(self.main_frame)\n",
    "        status_frame.pack(fill=tk.X)\n",
    "        \n",
    "        self.status_label = tk.Label(status_frame, text=\"Status: Ready\", font=('Helvetica', 12))\n",
    "        self.status_label.pack(side=tk.LEFT)\n",
    "        \n",
    "        # Console output\n",
    "        self.console = tk.Text(self.main_frame, height=8, state=tk.DISABLED)\n",
    "        self.console.pack(fill=tk.X, pady=10)\n",
    "        \n",
    "    def log_message(self, message):\n",
    "        self.console.config(state=tk.NORMAL)\n",
    "        self.console.insert(tk.END, message + \"\\n\")\n",
    "        self.console.config(state=tk.DISABLED)\n",
    "        self.console.see(tk.END)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.log_message(\"Loading violence detection model...\")\n",
    "        self.load_btn.config(state=tk.DISABLED)\n",
    "        \n",
    "        def load_model_thread():\n",
    "            try:\n",
    "                self.model = load_model('./model/LRCN_model__Date_Time_2025_05_06__22_51_48__Loss_0.9145573973655701__Accuracy__0.7631579041481018.h5')\n",
    "                self.log_message(\"Model loaded successfully!\")\n",
    "                self.select_btn.config(state=tk.NORMAL)\n",
    "            except Exception as e:\n",
    "                self.log_message(f\"Error loading model: {str(e)}\")\n",
    "                self.load_btn.config(state=tk.NORMAL)\n",
    "                \n",
    "        threading.Thread(target=load_model_thread, daemon=True).start()\n",
    "        \n",
    "    def select_video(self):\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select Video File\",\n",
    "            filetypes=[(\"Video Files\", \"*.mp4 *.avi *.mov\"), (\"All Files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            self.video_path = file_path\n",
    "            self.log_message(f\"Selected video: {os.path.basename(file_path)}\")\n",
    "            self.start_btn.config(state=tk.NORMAL)\n",
    "            \n",
    "    def start_processing(self):\n",
    "        if not hasattr(self, 'video_path'):\n",
    "            return\n",
    "            \n",
    "        self.running = True\n",
    "        self.start_btn.config(state=tk.DISABLED)\n",
    "        self.stop_btn.config(state=tk.NORMAL)\n",
    "        self.select_btn.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Reset buffers\n",
    "        self.frame_buffer = deque(maxlen=self.SEQ_LENGTH)\n",
    "        self.predictions = deque(maxlen=self.SEQ_LENGTH)\n",
    "        self.current_status = \"Processing...\"\n",
    "        \n",
    "        # Start processing thread\n",
    "        threading.Thread(target=self.process_video, daemon=True).start()\n",
    "        \n",
    "    def stop_processing(self):\n",
    "        self.running = False\n",
    "        self.start_btn.config(state=tk.NORMAL)\n",
    "        self.stop_btn.config(state=tk.DISABLED)\n",
    "        self.select_btn.config(state=tk.NORMAL)\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.log_message(\"Processing stopped by user\")\n",
    "        \n",
    "    def process_video(self):\n",
    "        self.cap = cv2.VideoCapture(self.video_path)\n",
    "        if not self.cap.isOpened():\n",
    "            self.log_message(\"Error opening video file\")\n",
    "            return\n",
    "            \n",
    "        total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "        self.log_message(f\"Starting processing ({total_frames} frames)...\")\n",
    "        \n",
    "        while self.running and self.cap.isOpened():\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                frame_count += 1\n",
    "                \n",
    "                # Process frame\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                display_frame = cv2.resize(frame_rgb, self.DISPLAY_SIZE)\n",
    "                \n",
    "                # Prepare frame for model\n",
    "                model_frame = cv2.resize(frame_rgb, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                model_frame = model_frame.astype(\"float32\") / 255.0\n",
    "                self.frame_buffer.append(model_frame)\n",
    "                \n",
    "                # When we have enough frames for a sequence\n",
    "                if len(self.frame_buffer) == self.SEQ_LENGTH and self.model:\n",
    "                    # Create sequence tensor\n",
    "                    sequence = np.expand_dims(np.array(self.frame_buffer), axis=0)\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    pred = self.model.predict(sequence, verbose=0)[0][0]\n",
    "                    self.predictions.append(pred)\n",
    "                    \n",
    "                    # Apply temporal smoothing\n",
    "                    if self.predictions:\n",
    "                        avg_prediction = np.mean(self.predictions)\n",
    "                        self.current_status = \"Violence\" if avg_prediction > self.VIOLENCE_THRESHOLD else \"Normal\"\n",
    "                        \n",
    "                # Update UI\n",
    "                self.update_display(display_frame)\n",
    "                \n",
    "                # Control processing speed\n",
    "                time.sleep(0.03)  # ~30fps\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_message(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "                break\n",
    "                \n",
    "        # Cleanup\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.running = False\n",
    "        self.log_message(f\"Finished processing {frame_count} frames\")\n",
    "        self.root.after(0, self.stop_processing)\n",
    "        \n",
    "    def update_display(self, frame):\n",
    "        # Add status text to frame\n",
    "        color = (255, 0, 0) if self.current_status == \"Violence\" else (0, 255, 0)\n",
    "        cv2.putText(frame, \n",
    "                   f\"{self.current_status}\", \n",
    "                   (35, 50), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, color, 2)\n",
    "        \n",
    "        # Convert to PhotoImage\n",
    "        img = Image.fromarray(frame)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        \n",
    "        # Update status label\n",
    "        self.status_label.config(text=f\"Status: {self.current_status}\")\n",
    "        \n",
    "        # Update video display\n",
    "        self.video_label.imgtk = imgtk\n",
    "        self.video_label.config(image=imgtk)\n",
    "        \n",
    "    def on_closing(self):\n",
    "        self.stop_processing()\n",
    "        self.root.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ViolenceDetectionUI(root)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 788 frames\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "\n",
    "# Constants\n",
    "SEQ_LENGTH = 1000  # Number of frames in each sequence\n",
    "IMG_SIZE = 64\n",
    "DISPLAY_SIZE = (512, 360)\n",
    "VIOLENCE_THRESHOLD = 0.56  # Confidence threshold for violence detection\n",
    "\n",
    "def process_full_video(video_path):\n",
    "    # Create output directory\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model('./model/LRCN_model__Date_Time_2025_05_06__22_51_48__Loss_0.9145573973655701__Accuracy__0.7631579041481018.h5')\n",
    "    \n",
    "    # Initialize frame buffer\n",
    "    frame_buffer = deque(maxlen=SEQ_LENGTH)\n",
    "    predictions = deque(maxlen=SEQ_LENGTH)  # Store recent predictions\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties for writer\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter('output.mp4', fourcc, fps, DISPLAY_SIZE)\n",
    "    \n",
    "    # Create display window\n",
    "    cv2.namedWindow(\"Violence Detection\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Violence Detection\", *DISPLAY_SIZE)\n",
    "    \n",
    "    frame_count = 0\n",
    "    current_status = \"Initializing...\"\n",
    "    current_confidence = 0.0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Process frame\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            display_frame = cv2.resize(frame_rgb, DISPLAY_SIZE)\n",
    "            \n",
    "            # Prepare frame for model\n",
    "            model_frame = cv2.resize(frame_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "            model_frame = model_frame.astype(\"float32\") / 255.0\n",
    "            frame_buffer.append(model_frame)\n",
    "            \n",
    "            # When we have enough frames for a sequence\n",
    "            if len(frame_buffer) == SEQ_LENGTH:\n",
    "                # Create sequence tensor (1, 15, 128, 128, 3)\n",
    "                sequence = np.expand_dims(np.array(frame_buffer), axis=0)\n",
    "                \n",
    "                # Make prediction\n",
    "                pred = model.predict(sequence, verbose=0)[0]\n",
    "                predictions.append(pred)\n",
    "                print(predictions)\n",
    "                # Apply temporal smoothing (average of last SEQ_LENGTH predictions)\n",
    "                avg_prediction = np.mean(predictions) if predictions else 0.0\n",
    "                is_violence = avg_prediction > VIOLENCE_THRESHOLD\n",
    "                current_status = \"Shoplifting\" if is_violence else \"No Shoplifting\"\n",
    "                current_confidence = avg_prediction\n",
    "            \n",
    "            # Add annotation to display frame (using current status)\n",
    "            color = (0, 0, 255) if current_status == \"Violence\" else (0, 255, 0)\n",
    "            cv2.putText(display_frame, \n",
    "                       f\"{current_status} ({current_confidence:.2f})\", \n",
    "                       (35, 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, color, 2)\n",
    "            \n",
    "            # Save text output (continuously updating)\n",
    "            with open(\"output.txt\", 'w') as f:\n",
    "                f.write(f\"{current_status} ({current_confidence:.2f})\")\n",
    "            \n",
    "            # Write and display frame\n",
    "            writer.write(cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imshow(\"Violence Detection\", cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            frame_count += 1\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Finished processing {frame_count} frames\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"./video/VIDEO-2025-05-06-17-11-33 (1).mp4\"\n",
    "process_full_video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
